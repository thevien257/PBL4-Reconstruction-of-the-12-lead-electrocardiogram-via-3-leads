{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG 12-Lead Reconstruction - Google Colab Training\n",
    "\n",
    "**Paper**: [AI-enhanced reconstruction of the 12-lead ECG via 3-leads](https://www.nature.com/articles/s41746-024-01193-7)\n",
    "\n",
    "This notebook uses the **exact original codebase** with **PTB-XL dataset**.\n",
    "\n",
    "## Steps:\n",
    "1. Check GPU\n",
    "2. Mount Google Drive & load PTB-XL dataset\n",
    "3. Get project code\n",
    "4. Load/Convert PTB-XL data\n",
    "5. Train model\n",
    "6. Test & Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = 'cuda:0'\n",
    "else:\n",
    "    print(\"WARNING: No GPU! Go to Runtime -> Change runtime type -> GPU\")\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive & Load PTB-XL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PTB-XL dataset from Google Drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Your dataset ZIP path on Google Drive\n",
    "DRIVE_ZIP = '/content/drive/MyDrive/IMLE-Net-Project/data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1.zip'\n",
    "\n",
    "# Extract to /content/data\n",
    "!mkdir -p /content/data\n",
    "!unzip -q -o \"{DRIVE_ZIP}\" -d /content/data/\n",
    "\n",
    "# Set PTBXL_PATH\n",
    "extracted = list(Path('/content/data').glob('ptb-xl-*'))\n",
    "PTBXL_PATH = str(extracted[0]) if extracted else None\n",
    "print(f\"PTBXL_PATH = {PTBXL_PATH}\")\n",
    "!ls {PTBXL_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Project Code & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone from GitHub and install dependencies\n",
    "import os\n",
    "PROJECT_DIR = '/content/ecg_reconstruction'\n",
    "\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    print(\"Cloning from GitHub...\")\n",
    "    !git clone https://github.com/scripps-research/ecg_reconstruction.git {PROJECT_DIR}\n",
    "else:\n",
    "    print(f\"Project already exists at {PROJECT_DIR}\")\n",
    "\n",
    "# Install wfdb with compatible pandas version\n",
    "!pip install -q \"pandas<3.0\" wfdb tqdm\n",
    "\n",
    "# Fix NumPy 2.0 compatibility (np.infty -> np.inf)\n",
    "import numpy as np\n",
    "if not hasattr(np, 'infty'):\n",
    "    np.infty = np.inf\n",
    "    print(\"Applied NumPy 2.0 compatibility fix (np.infty = np.inf)\")\n",
    "\n",
    "%cd {PROJECT_DIR}\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create general_ptbxl.py (adapts original code to use pickle instead of MongoDB)\n",
    "# This version properly handles MongoDB $in operator for batch queries\n",
    "\n",
    "general_ptbxl_code = '''\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class PickleCollection:\n",
    "    \"\"\"Mimics MongoDB collection interface using pickle files\"\"\"\n",
    "    def __init__(self, data_list):\n",
    "        # Store as list and create index for fast lookup\n",
    "        self.data = data_list\n",
    "        self.index = {item['ElementID']: item for item in data_list}\n",
    "    \n",
    "    def find_one(self, query):\n",
    "        \"\"\"Find a single document matching the query\"\"\"\n",
    "        if query is None:\n",
    "            return self.data[0] if self.data else None\n",
    "        \n",
    "        # Handle ElementID lookup\n",
    "        if 'ElementID' in query:\n",
    "            element_id = query['ElementID']\n",
    "            item = self.index.get(element_id)\n",
    "            if item:\n",
    "                return self._convert_item(item)\n",
    "            return None\n",
    "        \n",
    "        # Handle _id lookup (treat as ElementID)\n",
    "        if '_id' in query:\n",
    "            element_id = query['_id']\n",
    "            item = self.index.get(element_id)\n",
    "            if item:\n",
    "                return self._convert_item(item)\n",
    "            return None\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def find(self, query=None):\n",
    "        \"\"\"Find documents matching the query - supports MongoDB $in operator\"\"\"\n",
    "        if query is None:\n",
    "            return [self._convert_item(item) for item in self.data]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Handle ElementID with $in operator (batch query)\n",
    "        if 'ElementID' in query:\n",
    "            value = query['ElementID']\n",
    "            if isinstance(value, dict) and '$in' in value:\n",
    "                # Batch lookup using $in operator\n",
    "                element_ids = value['$in']\n",
    "                for eid in element_ids:\n",
    "                    item = self.index.get(eid)\n",
    "                    if item:\n",
    "                        results.append(self._convert_item(item))\n",
    "                return results\n",
    "            else:\n",
    "                # Single lookup\n",
    "                item = self.index.get(value)\n",
    "                if item:\n",
    "                    return [self._convert_item(item)]\n",
    "                return []\n",
    "        \n",
    "        # Handle _id with $in operator\n",
    "        if '_id' in query:\n",
    "            value = query['_id']\n",
    "            if isinstance(value, dict) and '$in' in value:\n",
    "                element_ids = value['$in']\n",
    "                for eid in element_ids:\n",
    "                    item = self.index.get(eid)\n",
    "                    if item:\n",
    "                        results.append(self._convert_item(item))\n",
    "                return results\n",
    "            else:\n",
    "                item = self.index.get(value)\n",
    "                if item:\n",
    "                    return [self._convert_item(item)]\n",
    "                return []\n",
    "        \n",
    "        # Handle $or operator\n",
    "        if '$or' in query:\n",
    "            for sub_query in query['$or']:\n",
    "                results.extend(self.find(sub_query))\n",
    "            return results\n",
    "        \n",
    "        # Generic field matching\n",
    "        for item in self.data:\n",
    "            match = True\n",
    "            for key, value in query.items():\n",
    "                if item.get(key) != value:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                results.append(self._convert_item(item))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _convert_item(self, item):\n",
    "        \"\"\"Convert numpy arrays to pd.Series for compatibility\"\"\"\n",
    "        item_copy = dict(item)\n",
    "        if 'lead' in item_copy:\n",
    "            item_copy['lead'] = {\n",
    "                k: pd.Series(v) if isinstance(v, np.ndarray) else v \n",
    "                for k, v in item_copy['lead'].items()\n",
    "            }\n",
    "        return item_copy\n",
    "    \n",
    "    def count_documents(self, query=None):\n",
    "        return len(self.find(query))\n",
    "\n",
    "_data_collection = None\n",
    "\n",
    "def get_collection(database_params_file=None):\n",
    "    \"\"\"Load data from pickle instead of MongoDB\"\"\"\n",
    "    global _data_collection\n",
    "    if _data_collection is None:\n",
    "        pickle_path = os.path.join(get_parent_folder(), 'Feature_map', 'Dataset', 'data_collection.pkl')\n",
    "        print(f\"Loading data collection from {pickle_path}...\")\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            _data_collection = pickle.load(f)\n",
    "        print(f\"Loaded {len(_data_collection)} records\")\n",
    "    return PickleCollection(_data_collection)\n",
    "\n",
    "def get_parent_folder():\n",
    "    return \"/content/Data/\"\n",
    "\n",
    "def remove_dir(folder: str):\n",
    "    try:\n",
    "        shutil.rmtree(folder)        \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def get_twelve_keys():\n",
    "    return ['I', 'II', 'III', 'aVL', 'aVR', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "\n",
    "def get_lead_keys(leads: str):\n",
    "    if leads == 'limb':\n",
    "        keys = ['I', 'II']\n",
    "    elif leads == 'limb+comb(v3+v4)':\n",
    "        keys = ['I', 'II', ['V3', 'V4']]\n",
    "    elif leads == 'limb+v2+v4':\n",
    "        keys = ['I', 'II', 'V2', 'V4']\n",
    "    elif leads == 'full_limb':\n",
    "        keys = ['I', 'II', 'III', 'aVL', 'aVR', 'aVF']\n",
    "    elif leads == 'limb+v1':\n",
    "        keys = ['I', 'II', 'V1']\n",
    "    elif leads == 'limb+v2':\n",
    "        keys = ['I', 'II', 'V2']\n",
    "    elif leads == 'limb+v3':\n",
    "        keys = ['I', 'II', 'V3']\n",
    "    elif leads == 'limb+v4':\n",
    "        keys = ['I', 'II', 'V4']\n",
    "    elif leads == 'limb+v5':\n",
    "        keys = ['I', 'II', 'V5']\n",
    "    elif leads == 'limb+v6':\n",
    "        keys = ['I', 'II', 'V6']\n",
    "    elif leads == 'precordial':\n",
    "        keys = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    elif leads == 'full':\n",
    "        keys = ['I', 'II', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown leads: {leads}\")\n",
    "    return keys\n",
    "\n",
    "def get_data_classes(dataset: str):\n",
    "    # For PTB-XL, use 'other' to load ALL data without class filtering\n",
    "    if dataset == 'other' or dataset == 'all':\n",
    "        data_classes = ['other']\n",
    "    elif dataset == 'infarct+other':\n",
    "        data_classes = ['st_elevation_or_infarct', 'other']\n",
    "    elif dataset == 'infarct+noninfarct':\n",
    "        data_classes = ['st_elevation_or_infarct', 'non_st_elevation_or_infarct']\n",
    "    else:\n",
    "        data_classes = ['other']  # Default to 'other' for PTB-XL\n",
    "    return data_classes\n",
    "\n",
    "def get_detect_classes(detect_class: str):\n",
    "    detect_classes = [detect_class]\n",
    "    return detect_classes\n",
    "\n",
    "def get_value_range():\n",
    "    min_value = -2.5\n",
    "    amplitude = 5.0\n",
    "    wave_sample = 2500\n",
    "    return min_value, amplitude, wave_sample\n",
    "'''\n",
    "\n",
    "with open('util_functions/general_ptbxl.py', 'w') as f:\n",
    "    f.write(general_ptbxl_code)\n",
    "print(\"Created util_functions/general_ptbxl.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load or Convert PTB-XL Data\n",
    "\n",
    "If data is already converted and saved to Google Drive, it will be loaded directly. Otherwise, conversion will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if converted data exists on Drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "DATA_DIR = '/content/Data'\n",
    "DRIVE_DATA_PATH = '/content/drive/MyDrive/ECG_Reconstruction/Data'\n",
    "\n",
    "# Remove any broken symlinks\n",
    "if os.path.islink(DATA_DIR):\n",
    "    os.remove(DATA_DIR)\n",
    "\n",
    "# Check if already converted and saved to Drive\n",
    "if os.path.exists(f'{DRIVE_DATA_PATH}/Feature_map/Dataset/data_collection.pkl'):\n",
    "    print(\"Found converted data on Drive! Copying...\")\n",
    "    !rm -rf {DATA_DIR}\n",
    "    !cp -r '{DRIVE_DATA_PATH}' {DATA_DIR}\n",
    "    print(\"Loaded from Drive (skipping conversion)\")\n",
    "    SKIP_CONVERSION = True\n",
    "else:\n",
    "    print(\"No converted data on Drive. Will convert from scratch.\")\n",
    "    !rm -rf {DATA_DIR}\n",
    "    !mkdir -p {DATA_DIR}\n",
    "    SKIP_CONVERSION = False\n",
    "\n",
    "print(f\"\\nData directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PTB-XL data (only runs if not loaded from Drive)\n",
    "if not SKIP_CONVERSION:\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import wfdb\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import gc\n",
    "\n",
    "    # Load database\n",
    "    df = pd.read_csv(f'{PTBXL_PATH}/ptbxl_database.csv', index_col='ecg_id')\n",
    "    df.scp_codes = df.scp_codes.apply(eval)\n",
    "    print(f\"Found {len(df)} records\")\n",
    "\n",
    "    lead_map = {'AVR': 'aVR', 'AVL': 'aVL', 'AVF': 'aVF'}\n",
    "    os.makedirs(f'{DATA_DIR}/Feature_map/Dataset', exist_ok=True)\n",
    "\n",
    "    # Process in chunks to avoid memory issues\n",
    "    CHUNK_SIZE = 2000\n",
    "    chunks = []\n",
    "\n",
    "    for chunk_idx, start in enumerate(range(0, len(df), CHUNK_SIZE)):\n",
    "        chunk_df = df.iloc[start:start+CHUNK_SIZE]\n",
    "        chunk_data = []\n",
    "        \n",
    "        for ecg_id, row in tqdm(chunk_df.iterrows(), total=len(chunk_df), desc=f\"Chunk {chunk_idx+1}\"):\n",
    "            try:\n",
    "                record_path = f\"{PTBXL_PATH}/{row['filename_hr']}\"\n",
    "                record = wfdb.rdrecord(record_path)\n",
    "                signal = record.p_signal\n",
    "                \n",
    "                lead_dict = {}\n",
    "                for i, name in enumerate(record.sig_name):\n",
    "                    mapped = lead_map.get(name, name)\n",
    "                    lead_dict[mapped] = (signal[:, i] * 1000).astype(np.float32)\n",
    "                \n",
    "                element = {\n",
    "                    'ElementID': f'ptbxl_{ecg_id}',\n",
    "                    'lead': lead_dict,\n",
    "                    'patient_id': row['patient_id']\n",
    "                }\n",
    "                chunk_data.append(element)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Save chunk\n",
    "        chunk_path = f'{DATA_DIR}/Feature_map/Dataset/chunk_{chunk_idx}.pkl'\n",
    "        with open(chunk_path, 'wb') as f:\n",
    "            pickle.dump(chunk_data, f, protocol=4)\n",
    "        print(f\"  Saved chunk {chunk_idx+1}: {len(chunk_data)} records\")\n",
    "        chunks.append(chunk_path)\n",
    "        del chunk_data\n",
    "        gc.collect()\n",
    "\n",
    "    # Combine chunks\n",
    "    print(\"Combining chunks...\")\n",
    "    all_data = []\n",
    "    for chunk_path in chunks:\n",
    "        with open(chunk_path, 'rb') as f:\n",
    "            all_data.extend(pickle.load(f))\n",
    "        os.remove(chunk_path)\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"Total records: {len(all_data)}\")\n",
    "\n",
    "    # Save final file\n",
    "    pkl_path = f'{DATA_DIR}/Feature_map/Dataset/data_collection.pkl'\n",
    "    print(\"Saving final file...\")\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump(all_data, f, protocol=4)\n",
    "    \n",
    "    size_mb = os.path.getsize(pkl_path) / 1024 / 1024\n",
    "    print(f\"Done! File size: {size_mb:.1f} MB\")\n",
    "\n",
    "    # Create train/valid/test splits\n",
    "    element_ids = [d['ElementID'] for d in all_data]\n",
    "    patient_map_dict = {d['ElementID']: d['patient_id'] for d in all_data}\n",
    "    patients = list(set(patient_map_dict.values()))\n",
    "    \n",
    "    train_p, temp_p = train_test_split(patients, test_size=0.3, random_state=42)\n",
    "    valid_p, test_p = train_test_split(temp_p, test_size=0.5, random_state=42)\n",
    "    \n",
    "    train_p_set, valid_p_set, test_p_set = set(train_p), set(valid_p), set(test_p)\n",
    "    train_ids = [e for e in element_ids if patient_map_dict[e] in train_p_set]\n",
    "    valid_ids = [e for e in element_ids if patient_map_dict[e] in valid_p_set]\n",
    "    test_ids = [e for e in element_ids if patient_map_dict[e] in test_p_set]\n",
    "    \n",
    "    print(f\"Split: Train={len(train_ids)}, Valid={len(valid_ids)}, Test={len(test_ids)}\")\n",
    "    \n",
    "    # Save Dataset maps\n",
    "    dataset_path = f'{DATA_DIR}/Feature_map/Dataset'\n",
    "    all_patients = list(patient_map_dict.values())\n",
    "    train_patients = [patient_map_dict[e] for e in train_ids]\n",
    "    valid_patients = [patient_map_dict[e] for e in valid_ids]\n",
    "    test_patients = [patient_map_dict[e] for e in test_ids]\n",
    "    \n",
    "    for name, data in [('map', element_ids), ('clean_map', element_ids), ('corrupted_map', []),\n",
    "                       ('train_map', train_ids), ('valid_map', valid_ids), ('test_map', test_ids),\n",
    "                       ('patient_map', all_patients), ('clean_patient_map', all_patients), ('corrupted_patient_map', []),\n",
    "                       ('train_patient_map', train_patients), ('valid_patient_map', valid_patients), ('test_patient_map', test_patients)]:\n",
    "        with open(f'{dataset_path}/{name}.pkl', 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    \n",
    "    # Create 'other' Dataclass (contains ALL data - required by the codebase)\n",
    "    other_path = f'{DATA_DIR}/Feature_map/Dataclass/other'\n",
    "    os.makedirs(other_path, exist_ok=True)\n",
    "    \n",
    "    for name, data in [('map', element_ids), ('clean_map', element_ids), ('corrupted_map', []),\n",
    "                       ('train_map', train_ids), ('valid_map', valid_ids), ('test_map', test_ids),\n",
    "                       ('patient_map', all_patients), ('clean_patient_map', all_patients), ('corrupted_patient_map', []),\n",
    "                       ('train_patient_map', train_patients), ('valid_patient_map', valid_patients), ('test_patient_map', test_patients)]:\n",
    "        with open(f'{other_path}/{name}.pkl', 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    \n",
    "    print(f\"Created 'other' Dataclass with {len(element_ids)} records\")\n",
    "    \n",
    "    del all_data\n",
    "    gc.collect()\n",
    "    print(\"Conversion complete!\")\n",
    "else:\n",
    "    print(\"Skipped conversion - data already loaded from Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create required map files if missing (needed when loading from Drive)\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_DIR = '/content/Data'\n",
    "dataset_path = f'{DATA_DIR}/Feature_map/Dataset'\n",
    "other_path = f'{DATA_DIR}/Feature_map/Dataclass/other'\n",
    "\n",
    "# Check if train_map.pkl exists in Dataset folder\n",
    "if not os.path.exists(f'{dataset_path}/train_map.pkl'):\n",
    "    print(\"Creating required map files...\")\n",
    "    \n",
    "    # Load data collection\n",
    "    with open(f'{dataset_path}/data_collection.pkl', 'rb') as f:\n",
    "        data_list = pickle.load(f)\n",
    "    print(f\"Loaded {len(data_list)} records\")\n",
    "    \n",
    "    # Extract IDs and patient info\n",
    "    element_ids = [d['ElementID'] for d in data_list]\n",
    "    patient_map_dict = {d['ElementID']: d['patient_id'] for d in data_list}\n",
    "    patients = list(set(patient_map_dict.values()))\n",
    "    \n",
    "    # Create train/valid/test splits\n",
    "    train_p, temp_p = train_test_split(patients, test_size=0.3, random_state=42)\n",
    "    valid_p, test_p = train_test_split(temp_p, test_size=0.5, random_state=42)\n",
    "    \n",
    "    train_p_set, valid_p_set, test_p_set = set(train_p), set(valid_p), set(test_p)\n",
    "    train_ids = [e for e in element_ids if patient_map_dict[e] in train_p_set]\n",
    "    valid_ids = [e for e in element_ids if patient_map_dict[e] in valid_p_set]\n",
    "    test_ids = [e for e in element_ids if patient_map_dict[e] in test_p_set]\n",
    "    \n",
    "    print(f\"Split: Train={len(train_ids)}, Valid={len(valid_ids)}, Test={len(test_ids)}\")\n",
    "    \n",
    "    # Patient lists\n",
    "    all_patients = list(patient_map_dict.values())\n",
    "    train_patients = [patient_map_dict[e] for e in train_ids]\n",
    "    valid_patients = [patient_map_dict[e] for e in valid_ids]\n",
    "    test_patients = [patient_map_dict[e] for e in test_ids]\n",
    "    \n",
    "    # All files to create\n",
    "    all_maps = [\n",
    "        ('map', element_ids), ('clean_map', element_ids), ('corrupted_map', []),\n",
    "        ('train_map', train_ids), ('valid_map', valid_ids), ('test_map', test_ids),\n",
    "        ('patient_map', all_patients), ('clean_patient_map', all_patients), ('corrupted_patient_map', []),\n",
    "        ('train_patient_map', train_patients), ('valid_patient_map', valid_patients), ('test_patient_map', test_patients)\n",
    "    ]\n",
    "    \n",
    "    # Save to Dataset folder\n",
    "    for name, data in all_maps:\n",
    "        with open(f'{dataset_path}/{name}.pkl', 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    print(f\"Saved {len(all_maps)} files to Dataset/\")\n",
    "    \n",
    "    # Create 'other' Dataclass folder\n",
    "    os.makedirs(other_path, exist_ok=True)\n",
    "    for name, data in all_maps:\n",
    "        with open(f'{other_path}/{name}.pkl', 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    print(f\"Created 'other' Dataclass\")\n",
    "    \n",
    "    del data_list\n",
    "    import gc\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"Map files already exist\")\n",
    "    # Still ensure 'other' Dataclass exists\n",
    "    if not os.path.exists(f'{other_path}/train_map.pkl'):\n",
    "        print(\"Creating 'other' Dataclass...\")\n",
    "        os.makedirs(other_path, exist_ok=True)\n",
    "        for filename in os.listdir(dataset_path):\n",
    "            if filename.endswith('.pkl') and filename != 'data_collection.pkl':\n",
    "                with open(f'{dataset_path}/{filename}', 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                with open(f'{other_path}/{filename}', 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "        print(\"Created 'other' Dataclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save converted data to Google Drive (run after first conversion)\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "DRIVE_DATA_PATH = '/content/drive/MyDrive/ECG_Reconstruction/Data'\n",
    "\n",
    "# Check if Drive data needs updating\n",
    "needs_update = os.path.exists(DATA_DIR) and (\n",
    "    not os.path.exists(DRIVE_DATA_PATH) or \n",
    "    not os.path.exists(f'{DRIVE_DATA_PATH}/Feature_map/Dataset/train_map.pkl') or\n",
    "    not os.path.exists(f'{DRIVE_DATA_PATH}/Feature_map/Dataclass/other/train_map.pkl')\n",
    ")\n",
    "\n",
    "if needs_update:\n",
    "    print(f\"Saving/updating Google Drive: {DRIVE_DATA_PATH}\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    if os.path.exists(DRIVE_DATA_PATH):\n",
    "        shutil.rmtree(DRIVE_DATA_PATH)\n",
    "    shutil.copytree(DATA_DIR, DRIVE_DATA_PATH)\n",
    "    print(\"Saved to Drive!\")\n",
    "else:\n",
    "    print(\"Data on Drive is up to date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data setup\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "print(\"Dataset contents:\")\n",
    "!ls -la {DATA_DIR}/Feature_map/Dataset/ | head -15\n",
    "\n",
    "print(\"\\nDataclass 'other' contents:\")\n",
    "!ls -la {DATA_DIR}/Feature_map/Dataclass/other/\n",
    "\n",
    "# Verify pickle file is valid\n",
    "pkl_path = f'{DATA_DIR}/Feature_map/Dataset/data_collection.pkl'\n",
    "try:\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f\"\\ndata_collection.pkl is valid! Contains {len(data)} records\")\n",
    "    print(f\"Sample ElementID: {data[0]['ElementID']}\")\n",
    "    del data\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: {e}\")\n",
    "    print(\"You need to re-run the conversion cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch original code to use PTB-XL adapter\n",
    "!cp util_functions/general.py util_functions/general_backup.py\n",
    "!cp util_functions/general_ptbxl.py util_functions/general.py\n",
    "\n",
    "print(\"Code patched to use PTB-XL data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration (Paper Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration based on Paper (Nature s41746-024-01193-7)\n",
    "\n",
    "CONFIG = {\n",
    "    'device': DEVICE,\n",
    "    \n",
    "    # INPUT/OUTPUT LEADS (Paper: I + II + V3 -> V1-V6)\n",
    "    'input_leads': 'limb+v3',     # I, II, V3 (3 leads)\n",
    "    'output_leads': 'precordial', # V1-V6 (6 leads)\n",
    "    \n",
    "    # Dataset - use 'other' to load ALL PTB-XL data\n",
    "    'dataset': 'other',\n",
    "    'data_size': 'max',\n",
    "    \n",
    "    # Network architecture (Paper: ResCNN blocks)\n",
    "    'input_channel': 32,\n",
    "    'middle_channel': 32,\n",
    "    'output_channel': 32,\n",
    "    'input_depth': 3,\n",
    "    'middle_depth': 2,\n",
    "    'output_depth': 3,\n",
    "    'input_kernel': 17,\n",
    "    'middle_kernel': 17,\n",
    "    'output_kernel': 17,\n",
    "    'use_residual': 'true',\n",
    "    \n",
    "    # Training parameters\n",
    "    'epochs': 200,\n",
    "    'batch_size': 16,\n",
    "    'optimizer_algorithm': 'adam',\n",
    "    'learning_rate': 0.000003,\n",
    "    'weight_decay': 0.001,\n",
    "    'momentum': 0.9,\n",
    "    'nesterov': True,\n",
    "    'prioritize_percent': 0,\n",
    "    'prioritize_size': 0,\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIGURATION (Paper Settings)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input:  {CONFIG['input_leads']} (I, II, V3)\")\n",
    "print(f\"Output: {CONFIG['output_leads']} (V1-V6)\")\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Dataset: {CONFIG['dataset']} (all PTB-XL data)\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Fix NumPy 2.0 compatibility before importing\n",
    "import numpy as np\n",
    "if not hasattr(np, 'infty'):\n",
    "    np.infty = np.inf\n",
    "\n",
    "from util_functions.general import get_parent_folder, get_data_classes, get_lead_keys\n",
    "from training_functions.single_reconstruction_manager import ReconstructionManager\n",
    "\n",
    "# Get settings\n",
    "parent_folder = get_parent_folder()\n",
    "data_classes = get_data_classes(CONFIG['dataset'])  # Returns ['other']\n",
    "sub_classes = []\n",
    "\n",
    "# Show lead configuration\n",
    "input_keys = get_lead_keys(CONFIG['input_leads'])\n",
    "output_keys = get_lead_keys(CONFIG['output_leads'])\n",
    "print(f\"Input leads ({len(input_keys)}): {input_keys}\")\n",
    "print(f\"Output leads ({len(output_keys)}): {output_keys}\")\n",
    "print(f\"Data classes: {data_classes}\")\n",
    "print(f\"Data folder: {parent_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Reconstruction Manager\n",
    "manager = ReconstructionManager(\n",
    "    parent_folder=parent_folder,\n",
    "    device=CONFIG['device'],\n",
    "    sub_classes=sub_classes,\n",
    "    input_leads=CONFIG['input_leads'],\n",
    "    output_leads=CONFIG['output_leads'],\n",
    "    data_classes=data_classes,\n",
    "    data_size=CONFIG['data_size'],\n",
    "    input_channel=CONFIG['input_channel'],\n",
    "    middle_channel=CONFIG['middle_channel'],\n",
    "    output_channel=CONFIG['output_channel'],\n",
    "    input_depth=CONFIG['input_depth'],\n",
    "    middle_depth=CONFIG['middle_depth'],\n",
    "    output_depth=CONFIG['output_depth'],\n",
    "    input_kernel=CONFIG['input_kernel'],\n",
    "    middle_kernel=CONFIG['middle_kernel'],\n",
    "    output_kernel=CONFIG['output_kernel'],\n",
    "    use_residual=CONFIG['use_residual'],\n",
    "    epochs=CONFIG['epochs'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    prioritize_percent=CONFIG['prioritize_percent'],\n",
    "    prioritize_size=CONFIG['prioritize_size'],\n",
    "    optimizer_algorithm=CONFIG['optimizer_algorithm'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    "    momentum=CONFIG['momentum'],\n",
    "    nesterov=CONFIG['nesterov']\n",
    ")\n",
    "\n",
    "print(\"Reconstruction Manager initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"Initializing model...\")\n",
    "manager.reset_model()\n",
    "print(\"Model initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading training and validation datasets...\")\n",
    "manager.load_dataset(train=True, valid=True)\n",
    "print(\"Datasets loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(f\"Starting training for {CONFIG['epochs']} epochs on {DEVICE}...\")\n",
    "print(\"=\"*50)\n",
    "manager.train()\n",
    "print(\"=\"*50)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release memory and plot\n",
    "manager.release_dataset()\n",
    "manager.plot_train_stats()\n",
    "manager.plot_valid_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and test\n",
    "print(\"Loading model for testing...\")\n",
    "manager.load_model()\n",
    "manager.load_dataset(test=True)\n",
    "\n",
    "print(\"Running tests...\")\n",
    "manager.test()\n",
    "\n",
    "manager.release_dataset()\n",
    "manager.plot_test_stats()\n",
    "print(\"Testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot examples\n",
    "manager.load_model()\n",
    "manager.plot_random_example(plot_format='png')\n",
    "print(\"Random examples plotted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error examples\n",
    "manager.load_test_stats()\n",
    "manager.plot_error_example(plot_format='png')\n",
    "print(\"Error examples plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "drive_path = '/content/drive/MyDrive/ECG_Reconstruction_Results'\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "# Copy output folder\n",
    "output_folder = f'{DATA_DIR}/Analysis'\n",
    "if os.path.exists(output_folder):\n",
    "    shutil.copytree(output_folder, f'{drive_path}/Analysis', dirs_exist_ok=True)\n",
    "    print(f\"Results saved to: {drive_path}\")\n",
    "\n",
    "print(\"\\nDone! Results saved to Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
